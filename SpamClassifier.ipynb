{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Spam Classifier\n",
    "\n",
    "To train the basics of Natural Language Processing (NLP), here is a little project about the spam/ham classification. \n",
    "\n",
    "As I'm closely following the *great* book _Python Machine Learning_ by Sebastian Raschka, the code is largely similar to its Chapter 8 _Applying Machine Learning to Sentiment Analysis_, where a movie review classifier is implemented. The **original aspect** is applying the method to another problem and dataset: spam detection from the Apache spam and ham e-mail corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Source: https://spamassassin.apache.org/old/publiccorpus/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning goals:\n",
    "- Form a dataset from thousands of text files\n",
    "- Inspect its structure\n",
    "- Preprocess and clean it\n",
    "- Implement a __Bag of Words Model__ using the __Count Vectorizer__\n",
    "- Correct for word relevancy with the __Term-Frequency-Inverse-Document-Frequency__ (tf-idf) technique\n",
    "- Train and optimise the **Logistic Regression** model\n",
    "- Evaluate performance on a test set\n",
    "- Compare with *out-of-core* learning\n",
    "- Try to deploy the model on a web server\n",
    "- Try and find clusters of topics within the spam e-mails\n",
    "\n",
    "## \"Business\" goal:\n",
    "- Distinguish spam from ham with possibly high _recall_ (i.e. maximising the chance of detecting spam) and _precision_ (i.e. minimising _false positive_ rate of non-spam misclassified as spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of *ham* files: 5052\n",
      "Number of *spam* files: 1002\n"
     ]
    }
   ],
   "source": [
    "HAM_FOLDER = \"spamassassin.apache.org/old/publiccorpus/easy_ham/\"\n",
    "SPAM_FOLDER = \"spamassassin.apache.org/old/publiccorpus/spam/\"\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "HAM_FILES = glob.glob(HAM_FOLDER+'*')\n",
    "SPAM_FILES = glob.glob(SPAM_FOLDER+'*')\n",
    "\n",
    "print(\"Number of *ham* files: %d\" % len(HAM_FILES))\n",
    "print(\"Number of *spam* files: %d\" % len(SPAM_FILES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some examples of spam and ham with a help of the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek(file_list, index, keywords=[], show_only_keywords=False):\n",
    "    \"\"\"Display content of the file `file_list[index]`.\n",
    "    Find keywords and display their location after the file content.\"\"\"\n",
    "    with open(file_list[index], 'r') as f:\n",
    "        # Remove end-of-line\n",
    "        lines = [line.strip() for line in f.readlines()]\n",
    "    res = []\n",
    "    for i, line in enumerate(lines):\n",
    "        if not show_only_keywords:\n",
    "            print(line)\n",
    "        for keyword in keywords:\n",
    "            if keyword in line:\n",
    "                res.append(f\"Line {i} has the keyword `{keyword}`: {line}\")\n",
    "    if len(res) > 0:\n",
    "        print(\"KEYWORD SEARCH RESULTS:\")\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From WebBuilders@YourService.com  Thu Sep 26 11:11:58 2002\n",
      "Return-Path: <WebBuilders@YourService.com>\n",
      "Delivered-To: zzzz@localhost.spamassassin.taint.org\n",
      "Received: from localhost (jalapeno [127.0.0.1])\n",
      "by zzzzason.org (Postfix) with ESMTP id D4FFF16F03\n",
      "for <zzzz@localhost>; Thu, 26 Sep 2002 11:11:55 +0100 (IST)\n",
      "Received: from jalapeno [127.0.0.1]\n",
      "by localhost with IMAP (fetchmail-5.9.0)\n",
      "for zzzz@localhost (single-drop); Thu, 26 Sep 2002 11:11:55 +0100 (IST)\n",
      "Received: from webnote.net (mail.webnote.net [193.120.211.219]) by\n",
      "dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id g8PISRC08752 for\n",
      "<zzzz@jmason.org>; Wed, 25 Sep 2002 19:28:27 +0100\n",
      "Received: from rack3.easydns.com (rack3.easydns.com [205.210.42.50]) by\n",
      "webnote.net (8.9.3/8.9.3) with ESMTP id TAA27238 for <zzzz@spamassassin.taint.org>;\n",
      "Wed, 25 Sep 2002 19:29:02 +0100\n",
      "Received: from 52.25.36.25 (216-91-88-22.biltmorecomm.com [216.91.88.22])\n",
      "by rack3.easydns.com (Postfix) with SMTP id 9821F4BE98 for\n",
      "<zzzz@spamassassin.taint.org>; Wed, 25 Sep 2002 14:28:58 -0400 (EDT)\n",
      "From: WebXperts@no.hostname.specified, Design@no.hostname.specified,\n",
      "\"Inc.\" <WebBuilders@YourService.com>\n",
      "To: zzzz@spamassassin.taint.org\n",
      "Reply-To: WebBuilders@YourService.com\n",
      "Subject: We Build The Internet.  WebXperts.com  (Design / Programming / Consultation)\n",
      "Date: Wed, 25 Sep 2002 14:25:38 -0400\n",
      "MIME-Version: 1.0\n",
      "Message-Id: <20020925182858.9821F4BE98@rack3.easydns.com>\n",
      "Content-Type: multipart/related; boundary=\"1d9d507a-d864-478d-ba24-917611e50332\"\n",
      "\n",
      "\n",
      "This is a multi-part message in MIME format\n",
      "--1d9d507a-d864-478d-ba24-917611e50332\n",
      "Content-Type: text/html; charset=iso-8859-1\n",
      "Content-Transfer-Encoding: quoted-printable\n",
      "\n",
      "<!-- If this flyer does not appear correctly and/or images do not appear, go =\n",
      "to the following in your browser window www.webxperts.com -->\n",
      "<html>\n",
      "<head>\n",
      "<title>WebXperts Design, Inc.    We Build The Internet.</title>\n",
      "<style type=3D\"text/css\"><!--body{font-family: Verdana; font-size:10px; =\n",
      "color:white;}td{font-family:  Verdana; font-size:10px; =\n",
      "color:white;}a{font-family: Verdana; font-size:10px; =\n",
      "color:cccccc;}a:hover{font-family: Verdana; font-size:10px; =\n",
      "color:yellow;}--></style>\n",
      "</head>\n",
      "<body bgcolor=3D\"#666666\"><div align=3D\"center\">\n",
      "<table width=3D\"720\"><tr><td><hr>\n",
      "If This Flyer does not appear correctly and/or images do not appear, please =\n",
      "click the following link: <a href=3D=\n",
      "\"http://www.webxperts.com\">http://www.webxperts.com</a>.<hr align=3D=\n",
      "\"center\"></td></tr></table>\n",
      "<table border=3D\"0\" cellpadding=3D\"0\" cellspacing=3D\"0\" width=3D\"720\">\n",
      "<tr>   <td><img src=3D\"http://www.eluxmedia.com/forms/spacer.gif\" width=3D=\n",
      "\"94\" height=3D\"1\" border=3D\"0\"></td>   <td><img src=3D=\n",
      "\"http://www.eluxmedia.com/forms/spacer.gif\" width=3D\"106\" height=3D\"1\" =\n",
      "border=3D\"0\"></td>   <td><img src=3D=\n",
      "\"http://www.eluxmedia.com/forms/spacer.gif\" width=3D\"47\" height=3D\"1\" =\n",
      "border=3D\"0\"></td>   <td><img src=3D=\n",
      "\"http://www.eluxmedia.com/forms/spacer.gif\" width=3D\"59\" height=3D\"1\" =\n",
      "border=3D\"0\"></td>   <td><img src=3D=\n",
      "\"http://www.eluxmedia.com/forms/spacer.gif\" width=3D\"119\" height=3D\"1\" =\n",
      "border=3D\"0\"></td>   <td><img src=3D=\n",
      "\"http://www.eluxmedia.com/forms/spacer.gif\" width=3D\"34\" height=3D\"1\" =\n",
      "border=3D\"0\"></td>   <td><img src=3D=\n",
      "\"http://www.eluxmedia.com/forms/spacer.gif\" width=3D\"73\" height=3D\"1\" =\n",
      "border=3D\"0\"></td>   <td><img src=3D=\n",
      "\"http://www.eluxmedia.com/forms/spacer.gif\" width=3D\"111\" height=3D\"1\" =\n",
      "border=3D\"0\"></td>   <td><img src=3D=\n",
      "\"http://www.eluxmedia.com/forms/spacer.gif\" width=3D\"77\" height=3D\"1\" =\n",
      "border=3D\"0\"></td>   <td><img src=3D=\n",
      "\"http://www.eluxmedia.com/forms/spacer.gif\" width=3D\"1\" height=3D\"1\" border=3D=\n",
      "\"0\"></td>  </tr>\n",
      "<tr>\n",
      "<td>\n",
      "<form action=3D\"http://www.webxperts.com/webxflyer2002.asp\" method=3D=\n",
      "\"post\"><input type=3D\"hidden\" name=3D\"email\" value=3D=\n",
      "\"zzzz@spamassassin.taint.org\"><input type=3D\"hidden\" name=3D\"m\" value=3D=\n",
      "\"sept2002\"><input type=3D\"hidden\" name=3D\"page\" value=3D\"webdesign\"><input =\n",
      "type=3D\"hidden\" name=3D\"typeof\" value=3D\"homes and apartments\"><INPUT TYPE=3D=\n",
      "\"image\" SRC=3D\"http://www.eluxmedia.com/forms/flyer2002_r1_c1.gif\" ALT=3D\"Web =\n",
      "Design\" ></form>\n",
      "</td>\n",
      "<td>\n",
      "<form action=3D\"http://www.webxperts.com/webxflyer2002.asp\" method=3D=\n",
      "\"post\"><input type=3D\"hidden\" name=3D\"email\" value=3D=\n",
      "\"zzzz@spamassassin.taint.org\"><input type=3D\"hidden\" name=3D\"m\" value=3D=\n",
      "\"sept2002\"><input type=3D\"hidden\" name=3D\"page\" value=3D\"app_dev\"><input =\n",
      "type=3D\"hidden\" name=3D\"typeof\" value=3D\"homes and apartments\"><INPUT TYPE=3D=\n",
      "\"image\" SRC=3D\"http://www.eluxmedia.com/forms/flyer2002_r1_c2.gif\" ALT=3D=\n",
      "\"Application Development\" ></form>\n",
      "</td>\n",
      "<td colspan=3D\"2\">\n",
      "<form action=3D\"http://www.webxperts.com/webxflyer2002.asp\" method=3D=\n",
      "\"post\"><input type=3D\"hidden\" name=3D\"email\" value=3D=\n",
      "\"zzzz@spamassassin.taint.org\"><input type=3D\"hidden\" name=3D\"m\" value=3D=\n",
      "\"sept2002\"><input type=3D\"hidden\" name=3D\"page\" value=3D\"webmain\"><input =\n",
      "type=3D\"hidden\" name=3D\"typeof\" value=3D\"homes and apartments\"><INPUT TYPE=3D=\n",
      "\"image\" SRC=3D\"http://www.eluxmedia.com/forms/flyer2002_r1_c3.gif\" ALT=3D=\n",
      "\"Website Maintenance\" ></form>\n",
      "</td>\n",
      "<td>\n",
      "<form action=3D\"http://www.webxperts.com/webxflyer2002.asp\" method=3D=\n",
      "\"post\"><input type=3D\"hidden\" name=3D\"email\" value=3D=\n",
      "\"zzzz@spamassassin.taint.org\"><input type=3D\"hidden\" name=3D\"m\" value=3D=\n",
      "\"sept2002\"><input type=3D\"hidden\" name=3D\"page\" value=3D\"guaranteed\"><input =\n",
      "type=3D\"hidden\" name=3D\"typeof\" value=3D\"homes and apartments\"><INPUT TYPE=3D=\n",
      "\"image\" SRC=3D\"http://www.eluxmedia.com/forms/flyer2002_r1_c5.gif\" ALT=3D=\n",
      "\"Guarenteed Site Placement\" ></form>\n",
      "</td>\n",
      "<td colspan=3D\"2\">\n",
      "<form action=3D\"http://www.webxperts.com/webxflyer2002.asp\" method=3D=\n",
      "\"post\"><input type=3D\"hidden\" name=3D\"email\" value=3D=\n",
      "\"zzzz@spamassassin.taint.org\"><input type=3D\"hidden\" name=3D\"m\" value=3D=\n",
      "\"sept2002\"><input type=3D\"hidden\" name=3D\"page\" value=3D\"ecommerce\"><input =\n",
      "type=3D\"hidden\" name=3D\"typeof\" value=3D\"homes and apartments\"><INPUT TYPE=3D=\n",
      "\"image\" SRC=3D\"http://www.eluxmedia.com/forms/flyer2002_r1_c6.gif\" ALT=3D=\n",
      "\"E-Commerce Solutions\" ></form>\n",
      "</td>\n",
      "<td>\n",
      "<form action=3D\"http://www.webxperts.com/webxflyer2002.asp\" method=3D=\n",
      "\"post\"><input type=3D\"hidden\" name=3D\"email\" value=3D=\n",
      "\"zzzz@spamassassin.taint.org\"><input type=3D\"hidden\" name=3D\"m\" value=3D=\n",
      "\"sept2002\"><input type=3D\"hidden\" name=3D\"page\" value=3D\"wireless\"><input =\n",
      "type=3D\"hidden\" name=3D\"typeof\" value=3D\"homes and apartments\"><INPUT TYPE=3D=\n",
      "\"image\" SRC=3D\"http://www.eluxmedia.com/forms/flyer2002_r1_c8.gif\" ALT=3D=\n",
      "\"Wireless Development\" ></form>\n",
      "</td>\n",
      "<td>\n",
      "<form action=3D\"http://www.webxperts.com/webxflyer2002.asp\" method=3D=\n",
      "\"post\"><input type=3D\"hidden\" name=3D\"email\" value=3D=\n",
      "\"zzzz@spamassassin.taint.org\"><input type=3D\"hidden\" name=3D\"m\" value=3D=\n",
      "\"sept2002\"><input type=3D\"hidden\" name=3D\"page\" value=3D\"intranets\"><input =\n",
      "type=3D\"hidden\" name=3D\"typeof\" value=3D\"homes and apartments\"><INPUT TYPE=3D=\n",
      "\"image\" SRC=3D\"http://www.eluxmedia.com/forms/flyer2002_r1_c9.gif\" ALT=3D=\n",
      "\"Intranets\" ></form>\n",
      "</td>\n",
      "<td><img src=3D\"http://www.eluxmedia.com/forms/spacer.gif\" width=3D\"1\" =\n",
      "height=3D\"33\" border=3D\"0\"></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td colspan=3D\"9\"><img name=3D\"logoandmiddle\" src=3D=\n",
      "\"http://www.eluxmedia.com/forms/logoandmiddle.gif\" width=3D\"720\" height=3D=\n",
      "\"150\" border=3D\"0\"></td>\n",
      "<td><img src=3D\"http://www.eluxmedia.com/forms/spacer.gif\" width=3D\"1\" =\n",
      "height=3D\"150\" border=3D\"0\"></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td colspan=3D\"3\">\n",
      "<form action=3D\"http://www.webxperts.com/webxflyer2002.asp\" method=3D=\n",
      "\"post\"><input type=3D\"hidden\" name=3D\"email\" value=3D=\n",
      "\"zzzz@spamassassin.taint.org\"><input type=3D\"hidden\" name=3D\"m\" value=3D=\n",
      "\"sept2002\"><input type=3D\"hidden\" name=3D\"page\" value=3D\"webdesign\"><input =\n",
      "type=3D\"hidden\" name=3D\"typeof\" value=3D\"homes and apartments\"><INPUT TYPE=3D=\n",
      "\"image\" SRC=3D\"http://www.eluxmedia.com/forms/flyer2002_r3_c1.gif\" ALT=3D=\n",
      "\"HTML Version\" ></form>\n",
      "</td>\n",
      "<td colspan=3D\"3\">\n",
      "<form action=3D\"http://www.webxperts.com/webxflyer2002.asp\" method=3D=\n",
      "\"post\"><input type=3D\"hidden\" name=3D\"email\" value=3D=\n",
      "\"zzzz@spamassassin.taint.org\"><input type=3D\"hidden\" name=3D\"m\" value=3D=\n",
      "\"sept2002\"><input type=3D\"hidden\" name=3D\"page\" value=3D\"mixedmedia\"><input =\n",
      "type=3D\"hidden\" name=3D\"typeof\" value=3D\"homes and apartments\"><INPUT TYPE=3D=\n",
      "\"image\" SRC=3D\"http://www.eluxmedia.com/forms/flyer2002_r3_c4.gif\" ALT=3D=\n",
      "\"Mixed Media Version\" ></form>\n",
      "</td>\n",
      "<td colspan=3D\"3\">\n",
      "<form action=3D\"http://www.webxperts.com/webxflyer2002.asp\" method=3D=\n",
      "\"post\"><input type=3D\"hidden\" name=3D\"email\" value=3D=\n",
      "\"zzzz@spamassassin.taint.org\"><input type=3D\"hidden\" name=3D\"m\" value=3D=\n",
      "\"sept2002\"><input type=3D\"hidden\" name=3D\"page\" value=3D\"flash\"><input type=3D=\n",
      "\"hidden\" name=3D\"typeof\" value=3D\"homes and apartments\"><INPUT TYPE=3D\"image\" =\n",
      "SRC=3D\"http://www.eluxmedia.com/forms/flyer2002_r3_c7.gif\" ALT=3D\"Flash =\n",
      "Version\" ></form>\n",
      "</td>\n",
      "<td><img src=3D\"http://www.eluxmedia.com/forms/spacer.gif\" width=3D\"1\" =\n",
      "height=3D\"277\" border=3D\"0\"></td>\n",
      "</tr>\n",
      "</table>\n",
      "<table width=3D\"720\"<tr><td><br><br><hr align=3D\"center\">\n",
      "Your email address was obtained from a purchased list. You are receiving this =\n",
      "from Eluxmedia LLC, and are a part of their mailing list. If you wish to =\n",
      "unsubscribe from this list, please <a href=3D=\n",
      "\"http://www.eluxmedia.com/unsub/unsubscribe.asp\">click here</a> and enter =\n",
      "your name into the remove box.<br><br>If you have previously unsubscribed and =\n",
      "are still receiving this message, you may email our <a href=3D=\n",
      "\"mailto:solutions@eluxmedia.com\">Abuse Control Center</a>.</font><hr align=3D=\n",
      "\"center\"></td></tr></table>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "--1d9d507a-d864-478d-ba24-917611e50332--\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peek(SPAM_FILES, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From rssfeeds@jmason.org  Tue Oct  1 10:36:33 2002\n",
      "Return-Path: <rssfeeds@example.com>\n",
      "Delivered-To: yyyy@localhost.example.com\n",
      "Received: from localhost (jalapeno [127.0.0.1])\n",
      "by jmason.org (Postfix) with ESMTP id 11E6316F56\n",
      "for <jm@localhost>; Tue,  1 Oct 2002 10:36:05 +0100 (IST)\n",
      "Received: from jalapeno [127.0.0.1]\n",
      "by localhost with IMAP (fetchmail-5.9.0)\n",
      "for jm@localhost (single-drop); Tue, 01 Oct 2002 10:36:05 +0100 (IST)\n",
      "Received: from dogma.slashnull.org (localhost [127.0.0.1]) by\n",
      "dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id g9181CK15579 for\n",
      "<jm@jmason.org>; Tue, 1 Oct 2002 09:01:12 +0100\n",
      "Message-Id: <200210010801.g9181CK15579@dogma.slashnull.org>\n",
      "To: yyyy@example.com\n",
      "From: fark <rssfeeds@example.com>\n",
      "Subject: Motel holds mans wheelchair ransom.\n",
      "Date: Tue, 01 Oct 2002 08:01:12 -0000\n",
      "Content-Type: text/plain; encoding=utf-8\n",
      "X-Spam-Status: No, hits=-572.3 required=5.0\n",
      "tests=AWL\n",
      "version=2.50-cvs\n",
      "X-Spam-Level:\n",
      "\n",
      "URL: http://www.newsisfree.com/click/-1,8410271,1717/\n",
      "Date: 2002-09-30T22:21:10+01:00\n",
      "\n",
      "[IMG: http://www.newsisfree.com/Images/fark/nandotimes.gif ([NandoTimes])]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peek(HAM_FILES, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice there might be a lot of html code in the e-mails.\n",
    "Perhaps spam has more html code to attract attention with colours and widgets.\n",
    "It seems the degree of html usage might help in classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\n",
      "==================================================\n",
      "KEYWORD SEARCH RESULTS:\n",
      "['Line 15 has the keyword `Subject:`: Subject: Motel holds mans wheelchair ransom.']\n",
      "KEYWORD SEARCH RESULTS:\n",
      "['Line 20 has the keyword `Subject:`: Subject: Re: [SAtalk] dependencies / pre-requisites for installation in']\n",
      "KEYWORD SEARCH RESULTS:\n",
      "['Line 32 has the keyword `Subject:`: Subject: Re: [SAtalk] getting single-user spam assassin to work in FreeBSD']\n",
      "KEYWORD SEARCH RESULTS:\n",
      "['Line 15 has the keyword `Subject:`: Subject: Scoble: \"I\\'m just not that excited by much that Microsoft is doing.\"']\n",
      "KEYWORD SEARCH RESULTS:\n",
      "['Line 27 has the keyword `Subject:`: Subject: Re: [SAtalk] Re: SA In The News']\n",
      "KEYWORD SEARCH RESULTS:\n",
      "['Line 15 has the keyword `Subject:`: Subject: Is rap music art? Rap song about farting answers that question']\n",
      "KEYWORD SEARCH RESULTS:\n",
      "['Line 28 has the keyword `Subject:`: Subject: Re: A moment of silence for the First Amendment (fwd)']\n",
      "KEYWORD SEARCH RESULTS:\n",
      "[\"Line 15 has the keyword `Subject:`: Subject: 'Nasty party' warning to Tories\"]\n",
      "KEYWORD SEARCH RESULTS:\n",
      "['Line 39 has the keyword `Subject:`: Subject: [zzzzteana] Latest Iraq-related news']\n",
      "KEYWORD SEARCH RESULTS:\n",
      "[\"Line 15 has the keyword `Subject:`: Subject: Don't do the brown WiFi, the brown WiFi is BAD\"]\n",
      "spam\n",
      "==================================================\n",
      "KEYWORD SEARCH RESULTS:\n",
      "['Line 22 has the keyword `Subject:`: Subject: We Build The Internet.  WebXperts.com  (Design / Programming / Consultation)']\n",
      "KEYWORD SEARCH RESULTS:\n",
      "['Line 35 has the keyword `Subject:`: Subject: [scoop] ....It is not my fault. .- vwiid']\n",
      "KEYWORD SEARCH RESULTS:\n",
      "['Line 23 has the keyword `Subject:`: Subject: The Government Grants You $25,000!']\n",
      "KEYWORD SEARCH RESULTS:\n",
      "['Line 16 has the keyword `Subject:`: Subject: Email Marketing']\n",
      "KEYWORD SEARCH RESULTS:\n",
      "['Line 22 has the keyword `Subject:`: Subject: Re: Super CHARGE your desktop or laptop today!     17639']\n",
      "KEYWORD SEARCH RESULTS:\n",
      "['Line 22 has the keyword `Subject:`: Subject: Best Long Distance On the Net - 3.9 Cents With No Monthly Fees']\n",
      "KEYWORD SEARCH RESULTS:\n",
      "['Line 19 has the keyword `Subject:`: Subject: FORTUNE 500 COMPANY HIRING, AT HOME REPS.']\n",
      "KEYWORD SEARCH RESULTS:\n",
      "['Line 17 has the keyword `Subject:`: Subject: A CRY FOR HELP']\n",
      "KEYWORD SEARCH RESULTS:\n",
      "['Line 19 has the keyword `Subject:`: Subject: How many \"inches\" do you need to satisfy a woman in bed?']\n",
      "KEYWORD SEARCH RESULTS:\n",
      "['Line 20 has the keyword `Subject:`: Subject: Make Huge Profits on eBay                          951']\n"
     ]
    }
   ],
   "source": [
    "for category in (\"ham\", \"spam\"):\n",
    "    print(category)\n",
    "    print(50*'=')\n",
    "    files = HAM_FILES if category=='ham' else SPAM_FILES\n",
    "    for i in range(10):\n",
    "        peek(files, i, [\"Subject:\"], show_only_keywords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many e-mails are an obvious spam or ham:\n",
    "- *spam*: the ones promising miracles or discussing \"measurements\" ;-)\n",
    "- *ham*: the opposite: no miracles but opinions or facts (the news); replies to questions\n",
    "\n",
    "Nonetheless, some message subjects are less obvious and even ham-sounding ones may have spam in the main body.\n",
    "Therefore, full message should be analysed.\n",
    "What is more, the e-mail headers with addresses might be of use too: the domain name or the user before '@' can be indicative of spam (e.g. if it is a random bundle of characters). So, we'll keep these too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Now we will form a pandas dataset from all the selected documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind # nice progress bar\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "try:\n",
    "    import magic # deduce file encoding\n",
    "except:\n",
    "    %pip install python-magic\n",
    "    import magic\n",
    "\n",
    "def deduce_encoding(file_name):\n",
    "    blob = open(file_name, 'rb').read()\n",
    "    m = magic.Magic(mime_encoding=True)\n",
    "    return m.from_buffer(blob)\n",
    "\n",
    "labels = {'spam': 1, 'ham': 0}\n",
    "def merge_data():\n",
    "    pbar = pyprind.ProgBar(len(SPAM_FILES)+len(HAM_FILES))\n",
    "    df = pd.DataFrame()\n",
    "    for l in ('spam', 'ham'):\n",
    "        files = SPAM_FILES if l=='spam' else HAM_FILES\n",
    "        for file in files:\n",
    "            try:\n",
    "                encoding = 'utf-8' # (For now use utf-8 only) alternatively: deduce_encoding(file)\n",
    "                #print(\"Encoding:\", encoding)\n",
    "                with open(file, 'r', encoding=encoding) as infile:\n",
    "                    txt = infile.read().replace('\\n', ' ')\n",
    "                df = df.append([[txt, labels[l]]],\n",
    "                                 ignore_index=True)\n",
    "                pbar.update()\n",
    "            except:\n",
    "                print(\"Problem with file\", file)\n",
    "                print(\"Skipping it\")\n",
    "                continue\n",
    "    df.columns = ['message', 'type']\n",
    "    print(df.head())\n",
    "    # Let's shuffle the dataset\n",
    "    np.random.seed(42)\n",
    "    df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "    # Save the dataset into a csv file\n",
    "    # Same encoding for all the messages may render some characters illegible\n",
    "    df.to_csv(\"spam_data.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "if not os.path.exists(\"spam_data.csv\"):\n",
    "    merge_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we accepted only the documents encoded using the utf-8 standard.\n",
    "Using the alternative, commented out, approach (`deduce_encoding(file)`) enables to extend a little the dataset, dropping only the files even the `magic` library has no spell for (e.g. binary or 8-bit files).\n",
    "However the latter option causes trouble in the topic modelling at the end of this notebook. Therefore the former way is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From fork-admin@xent.com  Sun Sep 22 23:59:01 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From fork-admin@xent.com  Wed Oct  2 16:02:24 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From pudge@perl.org  Thu Sep 26 11:02:41 2002 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From rssfeeds@jmason.org  Thu Oct  3 12:25:14 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From rpm-list-admin@freshrpms.net  Tue Oct  8 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>From rssfeeds@jmason.org  Mon Sep 30 13:44:06 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>From ilug-admin@linux.ie  Tue Oct  8 11:13:35 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>From fork-admin@xent.com  Sun Oct  6 22:57:39 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>From rpm-list-admin@freshrpms.net  Fri Sep  6 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>From rssfeeds@jmason.org  Mon Sep 30 13:43:38 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>From rssfeeds@jmason.org  Tue Oct  1 10:37:05 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>From rssfeeds@jmason.org  Mon Sep 30 13:36:55 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>From exmh-users-admin@redhat.com  Fri Sep 13 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>From fork-admin@xent.com  Mon Sep 23 18:32:29 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>From tony@svanstrom.com  Fri Aug 23 11:05:51 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              message  type\n",
       "0   From fork-admin@xent.com  Sun Sep 22 23:59:01 ...     0\n",
       "1   From fork-admin@xent.com  Wed Oct  2 16:02:24 ...     0\n",
       "2   From pudge@perl.org  Thu Sep 26 11:02:41 2002 ...     0\n",
       "3   From rssfeeds@jmason.org  Thu Oct  3 12:25:14 ...     0\n",
       "4   From rpm-list-admin@freshrpms.net  Tue Oct  8 ...     0\n",
       "5   From rssfeeds@jmason.org  Mon Sep 30 13:44:06 ...     0\n",
       "6   From ilug-admin@linux.ie  Tue Oct  8 11:13:35 ...     0\n",
       "7   From fork-admin@xent.com  Sun Oct  6 22:57:39 ...     0\n",
       "8   From rpm-list-admin@freshrpms.net  Fri Sep  6 ...     0\n",
       "9   From rssfeeds@jmason.org  Mon Sep 30 13:43:38 ...     0\n",
       "10  From rssfeeds@jmason.org  Tue Oct  1 10:37:05 ...     0\n",
       "11  From rssfeeds@jmason.org  Mon Sep 30 13:36:55 ...     0\n",
       "12  From exmh-users-admin@redhat.com  Fri Sep 13 1...     0\n",
       "13  From fork-admin@xent.com  Mon Sep 23 18:32:29 ...     0\n",
       "14  From tony@svanstrom.com  Fri Aug 23 11:05:51 2...     0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the csv\n",
    "df = pd.read_csv(\"spam_data.csv\", encoding='utf-8')\n",
    "\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5604, 2)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4769\n",
       "1     835\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14900071377587437"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['type'].sum() / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4769\n",
       "1     835\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5604, 2)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we saw that only 15% of data corresponds to spam.\n",
    "We should not be worried by that as long as the resulting classifier has decent both, precision and recall (sensitivity) (accuracy score is less interesting, since it is biased by the majority, non-spam, messages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words Model\n",
    "To distinguish spam from ham we are going to use Logistic Regression.\n",
    "However we cannot feed simply the e-mails into that algorithm.\n",
    "We will clean it and preprocess forming vectors of frequencies of _n-grams_ ($n$-length sequences of subsequent words in the document) with the Bag of Words Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is time to clean the data. To this end we will be using regular expressions (module `re`) and the Natural Language Toolkit (`nltk`). The latter one, in particular, contains a list of 'stop-words', i.e. not-so-meaningful words that should not be of much help to our classifier.\n",
    "\n",
    "Also, it seems pertinant to know how often the e-mails contain incorrect English.\n",
    "Perhaps spam has more spelling mistakes (or non-existing words).\n",
    "We will use the `enchant` module for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is word *I'm* correct English? True\n",
      "Is word *Hello!* correct English? False\n",
      "Is word *50* correct English? True\n",
      "Is word *$50* correct English? False\n",
      "Is word *ttypo* correct English? False\n",
      "Is word *hi@there* correct English? False\n",
      "Is word *html* correct English? True\n",
      "Is word *Thu* correct English? True\n",
      "Is word *Sep* correct English? True\n",
      "Is html correct now? True\n",
      "Is Jan correct English? True\n",
      "Is Feb correct English? True\n",
      "Is Mar correct English? True\n",
      "Is Apr correct English? True\n",
      "Is Mai correct English? True\n",
      "Is Jun correct English? True\n",
      "Is Jul correct English? True\n",
      "Is Aug correct English? True\n",
      "Is Sep correct English? True\n",
      "Is Oct correct English? True\n",
      "Is Nov correct English? True\n",
      "Is Dec correct English? True\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import enchant\n",
    "except:\n",
    "    %pip install pyenchant\n",
    "\n",
    "d = enchant.Dict(\"en_US\")\n",
    "def correct_english(word):\n",
    "    return d.check(word)\n",
    "for word in (\"I'm\", \"Hello!\", \"50\", \"$50\", \"ttypo\", \"hi@there\", \"html\", \"Thu\", \"Sep\"):\n",
    "    print(\"Is word *%s* correct English? %s\" %(word,correct_english(word)))\n",
    "d.add(\"html\")\n",
    "print(\"Is html correct now?\", correct_english(\"html\"))\n",
    "months = \"Jan Feb Mar Apr Mai Jun Jul Aug Sep Oct Nov Dec\".split()\n",
    "for m in months:\n",
    "    print(\"Is %s correct English? %s\" % (m, correct_english(m)))\n",
    "d.add(\"Sep\") # Oops. Just informed pyenchant developers about this bug :-)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package enchant:\n",
      "\n",
      "NAME\n",
      "    enchant\n",
      "\n",
      "DESCRIPTION\n",
      "    enchant:  Access to the enchant spellchecking library\n",
      "    =====================================================\n",
      "    \n",
      "    This module provides several classes for performing spell checking\n",
      "    via the Enchant spellchecking library.  For more details on Enchant,\n",
      "    visit the project website:\n",
      "    \n",
      "        https://abiword.github.io/enchant/\n",
      "    \n",
      "    Spellchecking is performed using 'Dict' objects, which represent\n",
      "    a language dictionary.  Their use is best demonstrated by a quick\n",
      "    example::\n",
      "    \n",
      "        >>> import enchant\n",
      "        >>> d = enchant.Dict(\"en_US\")   # create dictionary for US English\n",
      "        >>> d.check(\"enchant\")\n",
      "        True\n",
      "        >>> d.check(\"enchnt\")\n",
      "        False\n",
      "        >>> d.suggest(\"enchnt\")\n",
      "        ['enchant', 'enchants', 'enchanter', 'penchant', 'incant', 'enchain', 'enchanted']\n",
      "    \n",
      "    Languages are identified by standard string tags such as \"en\" (English)\n",
      "    and \"fr\" (French).  Specific language dialects can be specified by\n",
      "    including an additional code - for example, \"en_AU\" refers to Australian\n",
      "    English.  The later form is preferred as it is more widely supported.\n",
      "    \n",
      "    To check whether a dictionary exists for a given language, the function\n",
      "    'dict_exists' is available.  Dictionaries may also be created using the\n",
      "    function 'request_dict'.\n",
      "    \n",
      "    A finer degree of control over the dictionaries and how they are created\n",
      "    can be obtained using one or more 'Broker' objects.  These objects are\n",
      "    responsible for locating dictionaries for a specific language.\n",
      "    \n",
      "    Note that unicode strings are expected throughout the entire API.\n",
      "    Bytestrings should not be passed into any function.\n",
      "    \n",
      "    Errors that occur in this module are reported by raising subclasses\n",
      "    of 'Error'.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _enchant\n",
      "    checker (package)\n",
      "    errors\n",
      "    pypwl\n",
      "    tokenize (package)\n",
      "    utils\n",
      "\n",
      "SUBMODULES\n",
      "    _e\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        ProviderDesc\n",
      "    _EnchantObject(builtins.object)\n",
      "        Broker\n",
      "        Dict\n",
      "            DictWithPWL\n",
      "    \n",
      "    class Broker(_EnchantObject)\n",
      "     |  Broker object for the Enchant spellchecker.\n",
      "     |  \n",
      "     |  Broker objects are responsible for locating and managing dictionaries.\n",
      "     |  Unless custom functionality is required, there is no need to use Broker\n",
      "     |  objects directly. The 'enchant' module provides a default broker object\n",
      "     |  so that 'Dict' objects can be created directly.\n",
      "     |  \n",
      "     |  The most important methods of this class include:\n",
      "     |  \n",
      "     |      * :py:meth:`dict_exists`:   check existence of a specific language dictionary\n",
      "     |      * :py:meth:`request_dict`:  obtain a dictionary for specific language\n",
      "     |      * :py:meth:`set_ordering`:  specify which dictionaries to try for a given language.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Broker\n",
      "     |      _EnchantObject\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __del__(self)\n",
      "     |      Broker object destructor.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Customize pickling of PyEnchant objects.\n",
      "     |      \n",
      "     |      Since it's not safe for multiple objects to share the same C-library\n",
      "     |      object, we make sure it's unset when pickling.\n",
      "     |  \n",
      "     |  __init__(self)\n",
      "     |      Broker object constructor.\n",
      "     |      \n",
      "     |      This method is the constructor for the 'Broker' object.  No\n",
      "     |      arguments are required.\n",
      "     |  \n",
      "     |  describe(self)\n",
      "     |      Return list of provider descriptions.\n",
      "     |      \n",
      "     |      This method returns a list of descriptions of each of the\n",
      "     |      dictionary providers available.  Each entry in the list is a\n",
      "     |      ProviderDesc object.\n",
      "     |  \n",
      "     |  dict_exists(self, tag)\n",
      "     |      Check availability of a dictionary.\n",
      "     |      \n",
      "     |      This method checks whether there is a dictionary available for\n",
      "     |      the language specified by 'tag'.  It returns True if a dictionary\n",
      "     |      is available, and False otherwise.\n",
      "     |  \n",
      "     |  get_param(self, name)\n",
      "     |      Get the value of a named parameter on this broker.\n",
      "     |      \n",
      "     |      Parameters are used to provide runtime information to individual\n",
      "     |      provider backends.  See the method :py:meth:`set_param` for more details.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |      \n",
      "     |          This method does **not** work when using the Enchant C\n",
      "     |          library version 2.0 and above\n",
      "     |  \n",
      "     |  list_dicts(self)\n",
      "     |      Return list of available dictionaries.\n",
      "     |      \n",
      "     |      This method returns a list of dictionaries available to the\n",
      "     |      broker.  Each entry in the list is a two-tuple of the form:\n",
      "     |      \n",
      "     |          (tag,provider)\n",
      "     |      \n",
      "     |      where <tag> is the language lag for the dictionary and\n",
      "     |      <provider> is a ProviderDesc object describing the provider\n",
      "     |      through which that dictionary can be obtained.\n",
      "     |  \n",
      "     |  list_languages(self)\n",
      "     |      List languages for which dictionaries are available.\n",
      "     |      \n",
      "     |      This function returns a list of language tags for which a\n",
      "     |      dictionary is available.\n",
      "     |  \n",
      "     |  request_dict(self, tag=None)\n",
      "     |      Request a Dict object for the language specified by <tag>.\n",
      "     |      \n",
      "     |      This method constructs and returns a Dict object for the\n",
      "     |      requested language.  'tag' should be a string of the appropriate\n",
      "     |      form for specifying a language, such as \"fr\" (French) or \"en_AU\"\n",
      "     |      (Australian English).  The existence of a specific language can\n",
      "     |      be tested using the 'dict_exists' method.\n",
      "     |      \n",
      "     |      If <tag> is not given or is None, an attempt is made to determine\n",
      "     |      the current language in use.  If this cannot be determined, Error\n",
      "     |      is raised.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          this method is functionally equivalent to calling the Dict()\n",
      "     |          constructor and passing in the <broker> argument.\n",
      "     |  \n",
      "     |  request_pwl_dict(self, pwl)\n",
      "     |      Request a Dict object for a personal word list.\n",
      "     |      \n",
      "     |      This method behaves as 'request_dict' but rather than returning\n",
      "     |      a dictionary for a specific language, it returns a dictionary\n",
      "     |      referencing a personal word list.  A personal word list is a file\n",
      "     |      of custom dictionary entries, one word per line.\n",
      "     |  \n",
      "     |  set_ordering(self, tag, ordering)\n",
      "     |      Set dictionary preferences for a language.\n",
      "     |      \n",
      "     |      The Enchant library supports the use of multiple dictionary programs\n",
      "     |      and multiple languages.  This method specifies which dictionaries\n",
      "     |      the broker should prefer when dealing with a given language.  'tag'\n",
      "     |      must be an appropriate language specification and 'ordering' is a\n",
      "     |      string listing the dictionaries in order of preference.  For example\n",
      "     |      a valid ordering might be \"aspell,myspell,ispell\".\n",
      "     |      The value of 'tag' can also be set to \"*\" to set a default ordering\n",
      "     |      for all languages for which one has not been set explicitly.\n",
      "     |  \n",
      "     |  set_param(self, name, value)\n",
      "     |      Set the value of a named parameter on this broker.\n",
      "     |      \n",
      "     |      Parameters are used to provide runtime information to individual\n",
      "     |      provider backends.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |      \n",
      "     |          This method does **not** work when using the Enchant C\n",
      "     |          library version 2.0 and above\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _EnchantObject:\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _EnchantObject:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Dict(_EnchantObject)\n",
      "     |  Dict(tag=None, broker=None)\n",
      "     |  \n",
      "     |  Dictionary object for the Enchant spellchecker.\n",
      "     |  \n",
      "     |  Dictionary objects are responsible for checking the spelling of words\n",
      "     |  and suggesting possible corrections.  Each dictionary is owned by a\n",
      "     |  Broker object, but unless a new Broker has explicitly been created\n",
      "     |  then this will be the 'enchant' module default Broker and is of little\n",
      "     |  interest.\n",
      "     |  \n",
      "     |  The important methods of this class include:\n",
      "     |  \n",
      "     |      * check():              check whether a word id spelled correctly\n",
      "     |      * suggest():            suggest correct spellings for a word\n",
      "     |      * add():                add a word to the user's personal dictionary\n",
      "     |      * remove():             add a word to the user's personal exclude list\n",
      "     |      * add_to_session():     add a word to the current spellcheck session\n",
      "     |      * store_replacement():  indicate a replacement for a given word\n",
      "     |  \n",
      "     |  Information about the dictionary is available using the following\n",
      "     |  attributes:\n",
      "     |  \n",
      "     |      * tag:        the language tag of the dictionary\n",
      "     |      * provider:   a ProviderDesc object for the dictionary provider\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Dict\n",
      "     |      _EnchantObject\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __del__(self)\n",
      "     |      Dict object destructor.\n",
      "     |  \n",
      "     |  __init__(self, tag=None, broker=None)\n",
      "     |      Dict object constructor.\n",
      "     |      \n",
      "     |      A dictionary belongs to a specific language, identified by the\n",
      "     |      string <tag>.  If the tag is not given or is None, an attempt to\n",
      "     |      determine the language currently in use is made using the 'locale'\n",
      "     |      module.  If the current language cannot be determined, Error is raised.\n",
      "     |      \n",
      "     |      If <tag> is instead given the value of False, a 'dead' Dict object\n",
      "     |      is created without any reference to a language.  This is typically\n",
      "     |      only useful within PyEnchant itself.  Any other non-string value\n",
      "     |      for <tag> raises Error.\n",
      "     |      \n",
      "     |      Each dictionary must also have an associated Broker object which\n",
      "     |      obtains the dictionary information from the underlying system. This\n",
      "     |      may be specified using <broker>.  If not given, the default broker\n",
      "     |      is used.\n",
      "     |  \n",
      "     |  add(self, word)\n",
      "     |      Add a word to the user's personal word list.\n",
      "     |  \n",
      "     |  add_to_pwl(self, word)\n",
      "     |      Add a word to the user's personal word list.\n",
      "     |  \n",
      "     |  add_to_session(self, word)\n",
      "     |      Add a word to the session personal list.\n",
      "     |  \n",
      "     |  check(self, word)\n",
      "     |      Check spelling of a word.\n",
      "     |      \n",
      "     |      This method takes a word in the dictionary language and returns\n",
      "     |      True if it is correctly spelled, and false otherwise.\n",
      "     |  \n",
      "     |  is_added(self, word)\n",
      "     |      Check whether a word is in the personal word list.\n",
      "     |  \n",
      "     |  is_removed(self, word)\n",
      "     |      Check whether a word is in the personal exclude list.\n",
      "     |  \n",
      "     |  remove(self, word)\n",
      "     |      Add a word to the user's personal exclude list.\n",
      "     |  \n",
      "     |  remove_from_session(self, word)\n",
      "     |      Add a word to the session exclude list.\n",
      "     |  \n",
      "     |  store_replacement(self, mis, cor)\n",
      "     |      Store a replacement spelling for a miss-spelled word.\n",
      "     |      \n",
      "     |      This method makes a suggestion to the spellchecking engine that the\n",
      "     |      miss-spelled word <mis> is in fact correctly spelled as <cor>.  Such\n",
      "     |      a suggestion will typically mean that <cor> appears early in the\n",
      "     |      list of suggested spellings offered for later instances of <mis>.\n",
      "     |  \n",
      "     |  suggest(self, word)\n",
      "     |      Suggest possible spellings for a word.\n",
      "     |      \n",
      "     |      This method tries to guess the correct spelling for a given\n",
      "     |      word, returning the possibilities in a list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _EnchantObject:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Customize pickling of PyEnchant objects.\n",
      "     |      \n",
      "     |      Since it's not safe for multiple objects to share the same C-library\n",
      "     |      object, we make sure it's unset when pickling.\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _EnchantObject:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DictWithPWL(Dict)\n",
      "     |  DictWithPWL(tag, pwl=None, pel=None, broker=None)\n",
      "     |  \n",
      "     |  Dictionary with separately-managed personal word list.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |      As of version 1.4.0, enchant manages a per-user pwl and\n",
      "     |      exclude list.  This class is now only needed if you want\n",
      "     |      to explicitly maintain a separate word list in addition to\n",
      "     |      the default one.\n",
      "     |  \n",
      "     |  This class behaves as the standard Dict class, but also manages a\n",
      "     |  personal word list stored in a separate file.  The file must be\n",
      "     |  specified at creation time by the 'pwl' argument to the constructor.\n",
      "     |  Words added to the dictionary are automatically appended to the pwl file.\n",
      "     |  \n",
      "     |  A personal exclude list can also be managed, by passing another filename\n",
      "     |  to the constructor in the optional 'pel' argument.  If this is not given,\n",
      "     |  requests to exclude words are ignored.\n",
      "     |  \n",
      "     |  If either 'pwl' or 'pel' are None, an in-memory word list is used.\n",
      "     |  This will prevent calls to add() and remove() from affecting the user's\n",
      "     |  default word lists.\n",
      "     |  \n",
      "     |  The Dict object managing the PWL is available as the 'pwl' attribute.\n",
      "     |  The Dict object managing the PEL is available as the 'pel' attribute.\n",
      "     |  \n",
      "     |  To create a DictWithPWL from the user's default language, use None\n",
      "     |  as the 'tag' argument.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DictWithPWL\n",
      "     |      Dict\n",
      "     |      _EnchantObject\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, tag, pwl=None, pel=None, broker=None)\n",
      "     |      DictWithPWL constructor.\n",
      "     |      \n",
      "     |      The argument 'pwl', if not None, names a file containing the\n",
      "     |      personal word list.  If this file does not exist, it is created\n",
      "     |      with default permissions.\n",
      "     |      \n",
      "     |      The argument 'pel', if not None, names a file containing the personal\n",
      "     |      exclude list.  If this file does not exist, it is created with\n",
      "     |      default permissions.\n",
      "     |  \n",
      "     |  add(self, word)\n",
      "     |      Add a word to the associated personal word list.\n",
      "     |      \n",
      "     |      This method adds the given word to the personal word list, and\n",
      "     |      automatically saves the list to disk.\n",
      "     |  \n",
      "     |  add_to_pwl(self, word)\n",
      "     |      Add a word to the associated personal word list.\n",
      "     |      \n",
      "     |      This method adds the given word to the personal word list, and\n",
      "     |      automatically saves the list to disk.\n",
      "     |  \n",
      "     |  check(self, word)\n",
      "     |      Check spelling of a word.\n",
      "     |      \n",
      "     |      This method takes a word in the dictionary language and returns\n",
      "     |      True if it is correctly spelled, and false otherwise.  It checks\n",
      "     |      both the dictionary and the personal word list.\n",
      "     |  \n",
      "     |  is_added(self, word)\n",
      "     |      Check whether a word is in the personal word list.\n",
      "     |  \n",
      "     |  is_removed(self, word)\n",
      "     |      Check whether a word is in the personal exclude list.\n",
      "     |  \n",
      "     |  remove(self, word)\n",
      "     |      Add a word to the associated exclude list.\n",
      "     |  \n",
      "     |  suggest(self, word)\n",
      "     |      Suggest possible spellings for a word.\n",
      "     |      \n",
      "     |      This method tries to guess the correct spelling for a given\n",
      "     |      word, returning the possibilities in a list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Dict:\n",
      "     |  \n",
      "     |  __del__(self)\n",
      "     |      Dict object destructor.\n",
      "     |  \n",
      "     |  add_to_session(self, word)\n",
      "     |      Add a word to the session personal list.\n",
      "     |  \n",
      "     |  remove_from_session(self, word)\n",
      "     |      Add a word to the session exclude list.\n",
      "     |  \n",
      "     |  store_replacement(self, mis, cor)\n",
      "     |      Store a replacement spelling for a miss-spelled word.\n",
      "     |      \n",
      "     |      This method makes a suggestion to the spellchecking engine that the\n",
      "     |      miss-spelled word <mis> is in fact correctly spelled as <cor>.  Such\n",
      "     |      a suggestion will typically mean that <cor> appears early in the\n",
      "     |      list of suggested spellings offered for later instances of <mis>.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _EnchantObject:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Customize pickling of PyEnchant objects.\n",
      "     |      \n",
      "     |      Since it's not safe for multiple objects to share the same C-library\n",
      "     |      object, we make sure it's unset when pickling.\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _EnchantObject:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ProviderDesc(builtins.object)\n",
      "     |  ProviderDesc(name, desc, file)\n",
      "     |  \n",
      "     |  Simple class describing an Enchant provider.\n",
      "     |  \n",
      "     |  Each provider has the following information associated with it:\n",
      "     |  \n",
      "     |      * name:        Internal provider name (e.g. \"aspell\")\n",
      "     |      * desc:        Human-readable description (e.g. \"Aspell Provider\")\n",
      "     |      * file:        Location of the library containing the provider\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, pd)\n",
      "     |      Equality operator on ProviderDesc objects.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Hash operator on ProviderDesc objects.\n",
      "     |  \n",
      "     |  __init__(self, name, desc, file)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __slotnames__ = []\n",
      "\n",
      "FUNCTIONS\n",
      "    dict_exists(tag) method of Broker instance\n",
      "        Check availability of a dictionary.\n",
      "        \n",
      "        This method checks whether there is a dictionary available for\n",
      "        the language specified by 'tag'.  It returns True if a dictionary\n",
      "        is available, and False otherwise.\n",
      "    \n",
      "    get_enchant_version()\n",
      "        Get the version string for the underlying enchant library.\n",
      "    \n",
      "    get_param(name) method of Broker instance\n",
      "        Get the value of a named parameter on this broker.\n",
      "        \n",
      "        Parameters are used to provide runtime information to individual\n",
      "        provider backends.  See the method :py:meth:`set_param` for more details.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "            This method does **not** work when using the Enchant C\n",
      "            library version 2.0 and above\n",
      "    \n",
      "    get_user_config_dir()\n",
      "        Return the path that will be used by some\n",
      "        Enchant providers to look for custom dictionaries.\n",
      "    \n",
      "    list_dicts() method of Broker instance\n",
      "        Return list of available dictionaries.\n",
      "        \n",
      "        This method returns a list of dictionaries available to the\n",
      "        broker.  Each entry in the list is a two-tuple of the form:\n",
      "        \n",
      "            (tag,provider)\n",
      "        \n",
      "        where <tag> is the language lag for the dictionary and\n",
      "        <provider> is a ProviderDesc object describing the provider\n",
      "        through which that dictionary can be obtained.\n",
      "    \n",
      "    list_languages() method of Broker instance\n",
      "        List languages for which dictionaries are available.\n",
      "        \n",
      "        This function returns a list of language tags for which a\n",
      "        dictionary is available.\n",
      "    \n",
      "    request_dict(tag=None) method of Broker instance\n",
      "        Request a Dict object for the language specified by <tag>.\n",
      "        \n",
      "        This method constructs and returns a Dict object for the\n",
      "        requested language.  'tag' should be a string of the appropriate\n",
      "        form for specifying a language, such as \"fr\" (French) or \"en_AU\"\n",
      "        (Australian English).  The existence of a specific language can\n",
      "        be tested using the 'dict_exists' method.\n",
      "        \n",
      "        If <tag> is not given or is None, an attempt is made to determine\n",
      "        the current language in use.  If this cannot be determined, Error\n",
      "        is raised.\n",
      "        \n",
      "        .. note::\n",
      "            this method is functionally equivalent to calling the Dict()\n",
      "            constructor and passing in the <broker> argument.\n",
      "    \n",
      "    request_pwl_dict(pwl) method of Broker instance\n",
      "        Request a Dict object for a personal word list.\n",
      "        \n",
      "        This method behaves as 'request_dict' but rather than returning\n",
      "        a dictionary for a specific language, it returns a dictionary\n",
      "        referencing a personal word list.  A personal word list is a file\n",
      "        of custom dictionary entries, one word per line.\n",
      "    \n",
      "    set_param(name, value) method of Broker instance\n",
      "        Set the value of a named parameter on this broker.\n",
      "        \n",
      "        Parameters are used to provide runtime information to individual\n",
      "        provider backends.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "            This method does **not** work when using the Enchant C\n",
      "            library version 2.0 and above\n",
      "    \n",
      "    set_prefix_dir(path)\n",
      "        Set the prefix used by the Enchant library to find its plugins\n",
      "        \n",
      "        Called automatically when the Python library is imported when\n",
      "        required.\n",
      "\n",
      "VERSION\n",
      "    3.2.0\n",
      "\n",
      "FILE\n",
      "    /home/dawid/miniconda3/envs/datasc/lib/python3.8/site-packages/enchant/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(enchant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just saw only raw words and numbers, with no punctuation nor \"$\" sign are considered _correct_.\n",
    "We spotted that the dictionary does not recognise 'html', so we have added that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from profanity_filter import ProfanityFilter\n",
    "except:\n",
    "    %pip install profanity-filter\n",
    "# Also need: python spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = ProfanityFilter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Oh shit!** should be censored? True\n",
      "**I love you!** should be censored? False\n",
      "**Dick is a short for Richard** should be censored? True\n",
      "**Little cat is a pussy cat** should be censored? True\n"
     ]
    }
   ],
   "source": [
    "for w in (\"Oh shit!\", \"I love you!\", \"Dick is a short for Richard\", \"Little cat is a pussy cat\"):\n",
    "    print(\"**%s** should be censored? %s\" %(w, pf.is_profane(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subject: fekfwjlkej faeklj'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind=\"blababall Subject: fekfwjlkej faeklj\".find(\"Subject:\")\n",
    "\"blababall Subject: fekfwjlkej faeklj\"[ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "try:\n",
    "    import nltk\n",
    "except:\n",
    "    %pip install nltk\n",
    "try:\n",
    "    from nltk.corpus import stopwords\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "def preprocessor(text, lowercase=True, check_lang=True,\n",
    "                 skip_header=False):\n",
    "    \"\"\"Let's clean the message, tokenize the result and remove stop words\n",
    "    \n",
    "    Cleaning does the following:\n",
    "    - removing end-of-line characters and other non-word characters\n",
    "    - replace $ by `_dollar_`\n",
    "    - replacing HTML with `_HTML_` (spam has more HTML probably)\n",
    "    - replacing numbers with `_number_`\n",
    "    - replacing incorrect English with `_INCORRECT_ENG_`\n",
    "    \"\"\"\n",
    "    if skip_header:\n",
    "        ind = text.find(\"Subject:\")\n",
    "        if ind == -1:\n",
    "            return \"EMPTY\"\n",
    "        text = text[ind:]\n",
    "    text = re.sub(r'<[^>]*>', ' HTML ', text)\n",
    "    text = re.sub(r'\\$', ' DOLLAR ', text)\n",
    "    #text = ' '.join([w if pf.is_clean(w) else \" CENSURED \" for w in text.split()]) # takes long time\n",
    "    if check_lang:\n",
    "        text = ' '.join([w if correct_english(w) else \" INCORRECT_ENG \" for w in text.split()])\n",
    "    text = re.sub(r'[0-9]+\\.?[0-9]*', ' NUMBER ', text)\n",
    "    # remove non-words (NB. does not remove incorrect 'words')\n",
    "    text = re.sub(r'[\\W]+', ' ', text) \n",
    "    text = ' '.join([w for w in text.split() if w.lower() not in stop])\n",
    "    if lowercase:\n",
    "        return text.lower()\n",
    "    return text\n",
    "    #TODO: add params and use them in grid search(?)\n",
    "\n",
    "# Let's save altenative preprocessors as separate functions    \n",
    "from functools import partial\n",
    "preprocessor_keep_uppercase = partial(preprocessor, lowercase=False)\n",
    "preprocessor_no_lang_check = partial(preprocessor, check_lang=False)\n",
    "preprocessor_no_header_no_lang = partial(preprocessor, skip_header=True, check_lang=False)\n",
    "# import re\n",
    "# def preprocessor(text):\n",
    "#     \"\"\"Let's clean the message by:\n",
    "#     - removing end-of-line characters\n",
    "#     - replace $ by `dollar`\n",
    "#     - replacing HTML with `HTML` (spam has more HTML probably)\n",
    "#     \"\"\"\n",
    "#     text = re.sub('<[^>]*>', ' HTML ', text)\n",
    "#     text = re.sub('\\$', ' dollar ', text)\n",
    "#     #emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "#     # remove non-words and make the text lowercase (simplification)\n",
    "#     text = re.sub('[\\W]+', ' ', text.lower()) \n",
    "#     return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incorrect_eng incorrect_eng incorrect_eng dollar number\n",
      "give incorrect_eng dollar number\n",
      "give ttypo dollar number\n",
      "EMPTY\n"
     ]
    }
   ],
   "source": [
    "print(preprocessor(\"Oh, shit, I don't have ttypo $50\")) # \"I\" is a stop word\n",
    "print(preprocessor(\"I give you ttypo and $50\")) # 'you' and 'and' are stop words -> removed\n",
    "print(preprocessor_no_lang_check(\"I give you ttypo and $50\"))\n",
    "print(preprocessor_no_header_no_lang(\"I give you ttypo and $50\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WANT'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor_keep_uppercase(\"I WANT TO DO IT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'html'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(\"<email:bla@bla.com>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'html'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(\"<something\\>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thu number sep'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(\"Thu 25 Sep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From fork-admin@xent.com  Sun Sep 22 23:59:01 2002 Return-Path: <fork-admin@xent.com> Delivered-To: yyyy@localhost.spamassassin.taint.org Received: from localhost (jalapeno [127.0.0.1]) \\tby jmason.org (Postfix) with ESMTP id 3494216F03 \\tfor <jm@localhost>; Sun, 22 Sep 2002 23:59:01 +0100 (IST) Received: from jalapeno [127.0.0.1] \\tby localhost with IMAP (fetchmail-5.9.0) \\tfor jm@localhost (single-drop); Sun, 22 Sep 2002 23:59:01 +0100 (IST) Received: from xent.com ([64.161.22.236]) by dogma.slashnull.org     (8.11.6/8.11.6) with ESMTP id g8MMssC18527 for <jm@jmason.org>;     Sun, 22 Sep 2002 23:54:54 +0100 Received: from lair.xent.com (localhost [127.0.0.1]) by xent.com (Postfix)     with ESMTP id BB1302940A8; Sun, 22 Sep 2002 15:51:07 -0700 (PDT) Delivered-To: fork@spamassassin.taint.org Received: from mail.evergo.net (unknown [206.191.151.2]) by xent.com     (Postfix) with SMTP id B3EF729409A for <fork@xent.com>; Sun,     22 Sep 2002 15:50:06 -0700 (PDT) Received: (qmail 32383 invoked from network); 22 Sep 2002 22:53:43 -0000 Received: from dsl.206.191.151.102.evergo.net (HELO JMHALL)     (206.191.151.102) by mail.evergo.net with SMTP; 22 Sep 2002 22:53:43 -0000 Reply-To: <johnhall@evergo.net> From: \"John Hall\" <johnhall@evergo.net> To: \"FoRK\" <fork@spamassassin.taint.org> Subject: RE: sed /s/United States/Roman Empire/g Message-Id: <000f01c2628a$e71a61f0$0200a8c0@JMHALL> MIME-Version: 1.0 Content-Type: text/plain; charset=\"us-ascii\" Content-Transfer-Encoding: 7bit X-Priority: 3 (Normal) X-Msmail-Priority: Normal X-Mailer: Microsoft Outlook, Build 10.0.2627 Importance: Normal In-Reply-To: <DAV53oPB2EWrO8lxy4D00001559@hotmail.com> X-Mimeole: Produced By Microsoft MimeOLE V6.00.2600.0000 Sender: fork-admin@xent.com Errors-To: fork-admin@xent.com X-Beenthere: fork@spamassassin.taint.org X-Mailman-Version: 2.0.11 Precedence: bulk List-Help: <mailto:fork-request@xent.com?subject=help> List-Post: <mailto:fork@spamassassin.taint.org> List-Subscribe: <http://xent.com/mailman/listinfo/fork>, <mailto:fork-request@xent.com?subject=subscribe> List-Id: Friends of Rohit Khare <fork.xent.com> List-Unsubscribe: <http://xent.com/mailman/listinfo/fork>,     <mailto:fork-request@xent.com?subject=unsubscribe> List-Archive: <http://xent.com/pipermail/fork/> Date: Sun, 22 Sep 2002 15:53:43 -0700   > From: fork-admin@xent.com [mailto:fork-admin@xent.com] On Behalf Of Mr. > FoRK  > Also, the lifestyle of the remnants of those > societies is on average only marginally above poverty even today.  As I understand it, there is a huge difference between native Americans who speak english at home and those who do not.  I don\\'t have figures that separate those at hand, though.  1989 American Indians (US Pop as a whole) -- Families below poverty 27.2% (10%), Persons below poverty 31.2 (13.1), Speak a language other than English 23 (13.8) Married couple families 65.8 (79.5) Median family income $21,619 ($35,225) Per Capita $8,284 ($14,420).  Note: High Income countries in 1989 were defined as having over $6,000 per capita.  American Indians separated from the rest of the US society would still be considered a high-income society.        ']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"message\"].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['incorrect_eng wed oct number incorrect_eng number incorrect_eng html incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng id incorrect_eng html incorrect_eng incorrect_eng number oct number incorrect_eng incorrect_eng incorrect_eng incorrect_eng jalapeno incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng number oct number incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng id incorrect_eng html incorrect_eng incorrect_eng number oct number incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng id incorrect_eng incorrect_eng number oct number incorrect_eng number incorrect_eng incorrect_eng incorrect_eng incorrect_eng number number incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng id incorrect_eng html incorrect_eng incorrect_eng number oct number incorrect_eng number incorrect_eng incorrect_eng number number incorrect_eng incorrect_eng incorrect_eng number oct number incorrect_eng number incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng educational incorrect_eng incorrect_eng incorrect_eng number incorrect_eng incorrect_eng html incorrect_eng incorrect_eng incorrect_eng html incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng html incorrect_eng html incorrect_eng number incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng number number incorrect_eng bulk incorrect_eng html incorrect_eng html incorrect_eng html incorrect_eng html incorrect_eng friends incorrect_eng incorrect_eng html incorrect_eng html incorrect_eng html incorrect_eng html incorrect_eng incorrect_eng number oct number incorrect_eng number incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng sample conversation incorrect_eng messenger incorrect_eng incorrect_eng mike incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng incorrect_eng great incorrect_eng incorrect_eng oh freaking great adoption make incorrect_eng incorrect_eng add broken incorrect_eng shortened aol style phrasings incorrect_eng next thing incorrect_eng ai asking asl evil']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(df.loc[1, \"message\"]).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class Cleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, preprocessor):\n",
    "        self.preprocessor = preprocessor\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        cleaner_vec = np.vectorize(self.preprocessor)\n",
    "        return cleaner_vec(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for the Bag Of Words model, we need to get the _words_.\n",
    "The simplest way is to split the text by the usual word-delimiting characters, i.e. spaces, tabs, new-line characters, etc. An alternative is to use a 'stemmer' to ignore the word suffixes and, for example, treat _work_, _working_ and _worked_ as the same word. To this end we will be using the Porter Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'forest', 'run!!!', 'run', 'is', 'bore', 'and', 'caus', 'boredom.']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "def tokenizer(text):\n",
    "    \"Simple tokenizer\"\n",
    "    return text.split()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    \"Porter Tokenizer: use only stems of words\"\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "tokenizer_porter(\"Run Forest Run!!! Running is boring and causes boredom.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the Porter Stemmer is not an ideal approach: 'caus' should have been 'cause'.\n",
    "Nevertheless, it does reduce dimensionality of the problem.\n",
    "Regarding the punctuation marks, they are taken care of by the preprocessor already and will not be seen by the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['message'] = df['message'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take 80% of the dataset for a train test and 20% for a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8*df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4483"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:train_size, 'message'].values\n",
    "y_train = df.loc[:train_size, 'type'].values\n",
    "X_test = df.loc[train_size:, 'message'].values\n",
    "y_test = df.loc[train_size:, 'type'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency is how often a given term appears in a document (normalised to unity).\n",
    "It is a useful property, but an even better one is the _term frequency-inverse document frequency (tf-idf)_, which attributes a smaller importance to the words occurring across all the documents, focusing on the discriminating ones between the document classes.\n",
    "Below we will be using the tf-idf transformer `TfidfVectorizer` to create the Bag of Words model with the tf-idf technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  12 out of  12 | elapsed:  1.9min finished\n",
      "/home/dawid/miniconda3/envs/datasc/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', \"it'\", 'onc', 'onli', 'ourselv', \"she'\", \"should'v\", 'themselv', 'thi', 'veri', 'wa', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('preprocessor',\n",
       "                                        Cleaner(preprocessor=functools.partial(<function preprocessor at 0x7f9aad4f8430>, skip_header=True, check_lang=False))),\n",
       "                                       ('vect',\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       ('clf',\n",
       "                                        LogisticRegression(random_state=42,\n",
       "                                                           solver='liblinear'))]),\n",
       "             n_jobs=3,\n",
       "             param_grid=[{'clf__C': [100.0], 'clf__penalty': ['l1'],\n",
       "                          'vect__...\n",
       "                          'vect__stop_words': [['i', 'me', 'my', 'myself', 'we',\n",
       "                                                'our', 'ours', 'ourselves',\n",
       "                                                'you', \"you're\", \"you've\",\n",
       "                                                \"you'll\", \"you'd\", 'your',\n",
       "                                                'yours', 'yourself',\n",
       "                                                'yourselves', 'he', 'him',\n",
       "                                                'his', 'himself', 'she',\n",
       "                                                \"she's\", 'her', 'hers',\n",
       "                                                'herself', 'it', \"it's\", 'its',\n",
       "                                                'itself', ...],\n",
       "                                               None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x7f9ab5e56670>,\n",
       "                                              <function tokenizer_porter at 0x7f9a9d1f7040>]}],\n",
       "             scoring='f1', verbose=2)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False, # don't convert to lowercase\n",
    "                        preprocessor=None)\n",
    "param_grid = [\n",
    "              {'vect__ngram_range': [(3,3)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer,\n",
    "                                   tokenizer_porter],\n",
    "               'clf__penalty': ['l1'],\n",
    "               'clf__C': [100.0]},\n",
    "#               {'vect__ngram_range': [(1,1), (2,2), (3,3)],\n",
    "#                'vect__stop_words': [stop, None],\n",
    "#                'vect__tokenizer': [tokenizer,\n",
    "#                                    tokenizer_porter],\n",
    "#                'vect__use_idf': [False],\n",
    "#                'vect__norm': [None],\n",
    "#                'clf__penalty': ['l1', 'l2'],\n",
    "#                'clf__C': [100.0, 1000.0, 10000]},\n",
    "             ]\n",
    "prepare_and_model_pipeline = Pipeline([\n",
    "                     ('preprocessor', \n",
    "                          Cleaner(preprocessor_no_header_no_lang)),\n",
    "                     ('vect', tfidf),\n",
    "                     ('clf',\n",
    "                      LogisticRegression(random_state=42,\n",
    "                                         solver='liblinear'))])\n",
    "gs_lr_tfidf = GridSearchCV(prepare_and_model_pipeline,\n",
    "                           param_grid,\n",
    "                           scoring='f1',\n",
    "                           cv=3, verbose=2,\n",
    "                           n_jobs=3)\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 100.0, 'clf__penalty': 'l1', 'vect__ngram_range': (3, 3), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], 'vect__tokenizer': <function tokenizer_porter at 0x7f9a9d1f7040>} \n"
     ]
    }
   ],
   "source": [
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV F1 score: 0.985\n"
     ]
    }
   ],
   "source": [
    "print('CV F1 score: %.3f' % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"clf.pickle\", 'wb') as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Test Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model\n",
    "- [ ] try different classifiers\n",
    "- [ ] integrate preprocessing into the pipeline\n",
    "  - which tokenizer\n",
    "  - any special treatment of headers / html?\n",
    "- [ ] tackle overtraining\n",
    "- [ ] add more training samples (the trickier ones)\n",
    "- [X] out-of-core learning\n",
    "- [ ] Topic modelling: why this works (Dirichlet?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(clf, X, y, cv):\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import precision_score, recall_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    y_train_pred = cross_val_predict(estimator=clf, X=X, y=y, cv=cv)\n",
    "    print(\"Using %d fold of cross validation obtained the following metrics:\" % cv)\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y, y_train_pred))\n",
    "    print(\"Precision: %.2f\" % precision_score(y, y_train_pred))\n",
    "    print(\"Recall: %.2f\" % recall_score(y, y_train_pred))\n",
    "    print(\"F1 score: %.2f\" % f1_score(y, y_train_pred))\n",
    "\n",
    "def plot_precision_recall_vs_threshold(clf, X, y, cv, \n",
    "                                       yscale='linear', xlim=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    y_probas = cross_val_predict(estimator=clf, X=X, y=y, cv=cv, method='predict_proba')\n",
    "    y_scores = y_probas[:,1] # positive class (2nd column) probability\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y, y_scores)\n",
    "    \n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g--\", label=\"Recall\")\n",
    "    plt.legend()\n",
    "    plt.yscale(yscale)\n",
    "    if xlim:\n",
    "        plt.xlim(xlim)\n",
    "    plt.hlines(1, plt.xlim()[0], plt.xlim()[1], linestyle='dashed')\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"Fraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 3 fold of cross validation obtained the following metrics:\n",
      "Confusion matrix:\n",
      " [[3796    1]\n",
      " [   4  683]]\n",
      "Precision: 1.00\n",
      "Recall: 0.99\n",
      "F1 score: 1.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_metrics(clf, X_train, y_train, cv=3)\n",
    "plot_precision_recall_vs_threshold(clf, X_train, y_train, cv=3,\n",
    "                                   xlim=(0.1,0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like overtraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, on the test data\n",
    "#X_test_tr = gs_lr_tfidf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_tr.sum() / X_test_tr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 3 fold of cross validation obtained the following metrics:\n",
      "Confusion matrix:\n",
      " [[971   2]\n",
      " [  6 142]]\n",
      "Precision: 0.99\n",
      "Recall: 0.96\n",
      "F1 score: 0.97\n"
     ]
    }
   ],
   "source": [
    "print_metrics(clf, X_test, y_test, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_precision_recall_vs_threshold(clf, X_test, y_test, cv=3,\n",
    "                                   xlim=(0.0,1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pretend we have new messages\n",
    "X_new = np.array([\"Click the link below and you will win $1000\",\n",
    "                  \"<a> <ab> <ac> The US governemt has granted you $100000. Send your ID and credit card details.\",\n",
    "                  \"Hi John, how are you? Long time no see.\",\n",
    "                  \"Hey, I'm a sexy and intelligent girl. I like to hang out\",\n",
    "                  \"The construction bank of China offers you $10000 if you send me $1000\"])\n",
    "clf.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-core learning\n",
    "\n",
    "To speed up training and optimisation, we will use out-of-core learning, wherein the model is updated with a subset of the training documents.\n",
    "\n",
    "First, we need to encapsulate the pre-processing and tokenising steps into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "def cleaner(text):\n",
    "    \"\"\"Let's clean the message, tokenize the result and remove stop words\n",
    "    \n",
    "    Cleaning does the following:\n",
    "    - removing end-of-line characters and other non-word characters\n",
    "    - replace $ by `_dollar_`\n",
    "    - replacing HTML with `_HTML_` (spam has more HTML probably)\n",
    "    - replacing numbers with `_number_`\n",
    "    - replacing incorrect English with `_INCORRECT_ENG_`\n",
    "    \"\"\"\n",
    "    text = re.sub(r'<[^>]*>', ' _HTML_ ', text)\n",
    "    text = re.sub(r'\\$', ' _DOLLAR_ ', text)  \n",
    "    text = ' '.join([w if correct_english(w) else \" _INCORRECT_ENG_ \" for w in text.split()])\n",
    "    text = re.sub(r'[0-9]+\\.?[0-9]*', ' _NUMBER_ ', text)\n",
    "    # remove non-words (NB. does not remove incorrect 'words')\n",
    "    text = re.sub(r'[\\W]+', ' ', text) \n",
    "    text = [w for w in text.split() if w not in stop]\n",
    "    return ' '.join(text).lower()\n",
    "    #TODO: add params and use them in grid search(?)\n",
    "    \n",
    "def tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a generator that reads one document at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the above function to create a mini-batch of $n$ e-mails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For out-of-core learning we cannot use the `CountVectorizer`, but we may fall back on the `HashingVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=cleaner, # Let's try to use it (split cleaning and tokenizing?)\n",
    "                         tokenizer=tokenizer)\n",
    "clf = SGDClassifier(loss='log', random_state=42) # loss=log -> logistic regression\n",
    "doc_stream = stream_docs('spam_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [####] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:07\n"
     ]
    }
   ],
   "source": [
    "# Use 40 N_BATCHES * BATCH_SIZE documents for training\n",
    "N_BATCHES = 4\n",
    "BATCH_SIZE = 1000\n",
    "import pyprind\n",
    "pbar = pyprind.ProgBar(N_BATCHES)\n",
    "classes = np.array([0,1])\n",
    "for _ in range(N_BATCHES):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=BATCH_SIZE)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9241379310344827"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=1885) # TODO: hard-coded!\n",
    "X_test = vect.transform(X_test)\n",
    "clf.score(X_test, y_test)\n",
    "# TODO: can use recall and precision or f1 as score?\n",
    "# TODO: Can use pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How spam is caught? Topic modelling with Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('spam_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only spam\n",
    "df = df[df[\"type\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.pipeline import Pipeline\n",
    "count = CountVectorizer(stop_words='english', \n",
    "                        max_df=1.0,\n",
    "                        max_features=5000)\n",
    "lda = LatentDirichletAllocation(n_components=10,\n",
    "                                random_state=42,\n",
    "                                learning_method='batch',\n",
    "                                n_jobs=-1)\n",
    "pipe = Pipeline([('clean', Cleaner()),\n",
    "                 ('vect', count),\n",
    "                 ('lda', lda),\n",
    "                ])\n",
    "lda_topics = pipe.fit_transform(df['message'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4833)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1\n",
      "courteous breast enhancement sizes bust repaired upgrades engineer english bother russian intimidate www aver lift\n",
      "Topic 2\n",
      "courteous breast enhancement sizes bust repaired upgrades engineer english bother russian intimidate www aver lift\n",
      "Topic 3\n",
      "_incorrect_eng_ _number_ money business people make free mail grants government email send million report day\n",
      "Topic 4\n",
      "courteous breast enhancement sizes bust repaired upgrades engineer english bother russian intimidate www aver lift\n",
      "Topic 5\n",
      "_incorrect_eng_ _number_ id aug click free email microsoft receive information mail jalapeno message service list\n",
      "Topic 6\n",
      "courteous breast enhancement sizes bust repaired upgrades engineer english bother russian intimidate www aver lift\n",
      "Topic 7\n",
      "courteous breast enhancement sizes bust repaired upgrades engineer english bother russian intimidate www aver lift\n",
      "Topic 8\n",
      "courteous breast enhancement sizes bust repaired upgrades engineer english bother russian intimidate www aver lift\n",
      "Topic 9\n",
      "courteous breast enhancement sizes bust repaired upgrades engineer english bother russian intimidate www aver lift\n",
      "Topic 10\n",
      "courteous breast enhancement sizes bust repaired upgrades engineer english bother russian intimidate www aver lift\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 15\n",
    "feature_names = count.get_feature_names()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d\" % (topic_idx+1))\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()\\\n",
    "                    [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 8, 7])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(10)[:-3-1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: check spam filter on real-world examples (just for private curiosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
